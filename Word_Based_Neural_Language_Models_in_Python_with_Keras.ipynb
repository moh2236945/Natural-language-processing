{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word-Based Neural Language Models in Python with Keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUvAZpoEjTR8Nh0HqnqEi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2236945/Natural-language-processing/blob/master/Word_Based_Neural_Language_Models_in_Python_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhurFAWhhINu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmJ1NnQdhOKi",
        "colab_type": "text"
      },
      "source": [
        "Language modeling involves predicting the next word in a sequence given the sequence of words already present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9r8lGbehfN4",
        "colab_type": "text"
      },
      "source": [
        "Language models both learn and predict one word at a time. The training of the network involves providing sequences of words as input that are processed one at a time where a prediction can be made and learned for each input sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKEEV_TZhn-A",
        "colab_type": "text"
      },
      "source": [
        " 3 different ways of developing word-based language models in the Keras deep learning library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yUv5GbohY3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model 1: One-Word-In, One-Word-Out Sequences\n",
        "# source text\n",
        "data = \"\"\" Jack and Jill went up the hill\\n\n",
        "\t\tTo fetch a pail of water\\n\n",
        "\t\tJack fell down and broke his crown\\n\n",
        "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
        "#first step is to encode the text as integers.\n",
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MR9VftHilDG",
        "colab_type": "text"
      },
      "source": [
        "Word Embeddings for Text\n",
        "a type of word representation that allows words with similar meaning to have a similar representation.\n",
        "Word Embedding Algorithms\n",
        "three techniques that can be used to learn a word embedding from text data.\n",
        "1-Embedding Layer\n",
        "It requires that document text be cleaned and prepared such that each word is one-hot encoded. The size of the vector space is specified as part of the model, such as 50, 100, or 300 dimensions. The vectors are initialized with small random numbers. The embedding layer is used on the front end of a neural network and is fit in a supervised way using the Backpropagation algorithm.\n",
        "2. Word2Vec :\n",
        "a statistical method for efficiently learning a standalone word embedding from a text corpus.\n",
        "Two different learning models \n",
        "2-1 Continuous Bag-of-Words, or CBOW model.\n",
        "2-2Continuous Skip-Gram Model.\n",
        "3-GloVe\n",
        " is an extension to the word2vec method for efficiently learning word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SEgMivSkVAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#know the size of the vocabulary later for both defining the word embedding layer\n",
        "# determine the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8HBVogzkrGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step 3 :create sequences of words to fit the model with one word as input and one word as output\n",
        "# create word -> word sequences\n",
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "\tsequence = encoded[i-1:i+1]\n",
        "\tsequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc1L_6qrk06x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into X and y elements\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jJ3NkBfk50B",
        "colab_type": "text"
      },
      "source": [
        "fit our model to predict a probability distribution across all words in the vocabulary. That means that we need to turn the output element from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word that the value. This gives the network a ground truth to aim for from which we can calculate error and update the model.\n",
        "\n",
        "Keras provides the to_categorical() function that we can use to convert the integer to a one hot encoding while specifying the number of classes as the vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqibMUoTk_qA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode outputs\n",
        "y = to_categorical(y, num_classes=vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGc7saFjlDl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The model has a single hidden LSTM layer with 50 units\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idKx_2rSlOIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(X, y, epochs=500, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIMsyDmmlQgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate\n",
        "in_text = 'Jack'\n",
        "print(in_text)\n",
        "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "encoded = array(encoded)\n",
        "yhat = model.predict_classes(encoded, verbose=0)\n",
        "for word, index in tokenizer.word_index.items():\n",
        "\tif index == yhat:\n",
        "\t\tprint(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j7o_fpflilS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Full \n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        " \n",
        "# generate a sequence from the model\n",
        "def generate_seq(model, tokenizer, seed_text, n_words):\n",
        "\tin_text, result = seed_text, seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\tencoded = array(encoded)\n",
        "\t\t# predict a word in the vocabulary\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text, result = out_word, result + ' ' + out_word\n",
        "\treturn result\n",
        " \n",
        "# source text\n",
        "data = \"\"\" Jack and Jill went up the hill\\n\n",
        "\t\tTo fetch a pail of water\\n\n",
        "\t\tJack fell down and broke his crown\\n\n",
        "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded = tokenizer.texts_to_sequences([data])[0]\n",
        "# determine the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# create word -> word sequences\n",
        "sequences = list()\n",
        "for i in range(1, len(encoded)):\n",
        "\tsequence = encoded[i-1:i+1]\n",
        "\tsequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "# split into X and y elements\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,0],sequences[:,1]\n",
        "# one hot encode outputs\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(X, y, epochs=500, verbose=2)\n",
        "# evaluate\n",
        "print(generate_seq(model, tokenizer, 'Jack', 6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_jlnCS6lp0Z",
        "colab_type": "text"
      },
      "source": [
        "Model 2: Line-by-Line Sequence\n",
        " split up the source text line-by-line, then break each line down into a series of words that build up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdweQp2Sl9o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First create the sequences of integers, line-by-line by using the Tokenizer already fit on the source text."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azsVP_TlmAaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create line-based sequences\n",
        "sequences = list()\n",
        "for line in data.split('\\n'):\n",
        "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(encoded)):\n",
        "\t\tsequence = encoded[:i+1]\n",
        "\t\tsequences.append(sequence)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA0OtGxlmF62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Step2:pad the prepared sequences. We can do this using the pad_sequences() function provided in Keras.\n",
        "# This first involves finding the longest sequence, then using that as the length by which to pad-out all other sequences.\n",
        "# pad input sequences\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
        "print('Max Sequence Length: %d' % max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ByMcpsamW5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step 3 : split the sequences into input and output elements,\n",
        "# split into input and output elements\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1],sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgbZkyYemhaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(X, y, epochs=500, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxNq6ehymr91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pre-pad sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}