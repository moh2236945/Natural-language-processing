{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prepare a Photo Caption Dataset for Training a Deep Learning Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4a9j2xTGR3nLhec8+W8ap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2236945/Natural-language-processing/blob/master/Prepare_a_Photo_Caption_Dataset_for_Training_a_Deep_Learning_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYc12CIAzC8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defination and Description \n",
        "#https://www.jair.org/media/3994/live-3994-7274-jair.pdf\n",
        "from os import listdir\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        " \n",
        "def load_photos(directory):\n",
        "\timages = dict()\n",
        "\tfor name in listdir(directory):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for the model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare the image for the VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\timages[image_id] = image\n",
        "\treturn images\n",
        " \n",
        "# load images\n",
        "directory = 'Flicker8k_Dataset'\n",
        "images = load_photos(directory)\n",
        "print('Loaded Images: %d' % len(images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INGJcScG3f59",
        "colab_type": "text"
      },
      "source": [
        "Pre-Calculate Photo Features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLgKTbZ3i6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "# load the model\n",
        "in_layer = Input(shape=(224, 224, 3))\n",
        "model = VGG16(include_top=False, input_tensor=in_layer, pooling='avg')\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdUPn_Il3qQT",
        "colab_type": "text"
      },
      "source": [
        "**Load image description**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgGqzvqf3vxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# store the first description for each image\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = image_desc\n",
        "\treturn mapping\n",
        " \n",
        "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "doc = load_doc(filename)\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsHDaZ0n39zb",
        "colab_type": "text"
      },
      "source": [
        "**Preparing** **text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Soih8n74CAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        " \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# store the first description for each image\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = image_desc\n",
        "\treturn mapping\n",
        " \n",
        "# clean description text\n",
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\t# tokenize\n",
        "\t\tdesc = desc.split()\n",
        "\t\t# convert to lower case\n",
        "\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t# remove punctuation from each token\n",
        "\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t# remove hanging 's' and 'a'\n",
        "\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t# store as string\n",
        "\t\tdescriptions[key] =  ' '.join(desc)\n",
        " \n",
        "# save descriptions to file, one per line\n",
        "def save_doc(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        " \n",
        "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)\n",
        "# summarize vocabulary\n",
        "all_tokens = ' '.join(descriptions.values()).split()\n",
        "vocabulary = set(all_tokens)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "# save descriptions\n",
        "save_doc(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}