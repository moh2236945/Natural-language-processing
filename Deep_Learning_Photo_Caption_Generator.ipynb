{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Photo Caption Generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJ2AoRbguPWoq9+nqi0ipO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2236945/Natural-language-processing/blob/master/Deep_Learning_Photo_Caption_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACr8WBQMpKmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "#extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "    #load model\n",
        "    model=VGG16()\n",
        "    #re-strcture the model\n",
        "    model=layer.pop()\n",
        "    model=Model(inputs=model.input,output=model.layer[-1].output)\n",
        "    #extract features from each image\n",
        "    features=dict()\n",
        "    for name in listdir(directory):\n",
        "        #load an image file\n",
        "        filename=dirextory+'/'+name\n",
        "        image=load_img(filename,target_size=(224,224))\n",
        "        #convert image to numpy array\n",
        "        image=img_to_array(image)\n",
        "        #reshape data for the model\n",
        "        image=image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n",
        "        #prepare image for the vgg model\n",
        "        image=preprocess_input(image)\n",
        "        #get features\n",
        "        features=model.predict(image,verbos=0)\n",
        "        #get image Id\n",
        "        image_id=name.split('.')[0]\n",
        "        #store features\n",
        "        features[image_id]=feature\n",
        "    return features\n",
        "# extract features from all images\n",
        "directory = 'Flickr8k_Dataset'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZA17rI_sORn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#step2 Preparing text data\n",
        "#clean text\n",
        "def load_doc(filename):\n",
        "    file=open(filename,'r')\n",
        "    text=file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm5FJf0qsxL8",
        "colab_type": "text"
      },
      "source": [
        "Each photo has a unique identifier. This identifier is used on the photo filename and in the text file of descriptions.\n",
        "\n",
        "Next, we will step through the list of photo descriptions. Below defines a function load_descriptions() that, given the loaded document text, will return a dictionary of photo identifiers to descriptions. Each photo identifier maps to a list of one or more textual descriptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qfS_zXszzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract description for each image\n",
        "def load_description(doc):\n",
        "    mapping=dict()\n",
        "    #process lline\n",
        "    for line in doc.split('\\n'):\n",
        "        #spplit line by white space\n",
        "        tokens=line.split()\n",
        "        if len(line)<2:\n",
        "            continue\n",
        "        #take the first token as the imageid, the rest as description\n",
        "        image_id,imge_dec=token[0],token[1:]\n",
        "        #remove file name from image id\n",
        "        image_id=image_id.split('.')[0]\n",
        "        #convert description to string\n",
        "        image_desc='.'join(image_desc)\n",
        "        #create list \n",
        "        if image_id not in mapping:\n",
        "            mapping[image_id]=list()\n",
        "        #store decsription\n",
        "        mapping[image_id].append(image_desc)\n",
        "    return mapping\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8bvmoRCu0o5",
        "colab_type": "text"
      },
      "source": [
        "clean the description text. The descriptions are already tokenized and easy to work with.\n",
        "\n",
        "We will clean the text in the following ways in order to reduce the size of the vocabulary of words we will need to work with:\n",
        "\n",
        "Convert all words to lowercase.\n",
        "Remove all punctuation.\n",
        "Remove all words that are one character or less in length (e.g. ‘a’).\n",
        "Remove all words with numbers in them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-21yHRNu13P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        " \n",
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
        " \n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56-GHP3ZvBfV",
        "colab_type": "text"
      },
      "source": [
        " transform the clean descriptions into a set and print its size to get an idea of the size of our dataset vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esgiuPkBvCb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build a list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        " \n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKobFWKtvISO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        " \n",
        "# save descriptions\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xo799EHvbHM",
        "colab_type": "text"
      },
      "source": [
        "Develop Deep Learning **Model** **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjp811S4vd5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import load\n",
        " \n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        " \n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        " \n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features\n",
        " \n",
        "# load training dataset (6K)\n",
        "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bafF_upkv0j1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        " \n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1K639mDv6TO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM9hJoIBv98s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxVdT7l4wHKL",
        "colab_type": "text"
      },
      "source": [
        "define a deep learning based on the “merge-model” described by Marc Tanti, et al. in their 2017 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_2ZtvaJwLpD",
        "colab_type": "text"
      },
      "source": [
        "We will describe the model in three parts:\n",
        "\n",
        "**Photo Feature Extractor**. This is a 16-layer VGG model pre-trained on the ImageNet dataset. We have pre-processed the photos with the VGG model (without the output layer) and will use the extracted features predicted by this model as input.\n",
        "**Sequence Processor**. This is a word embedding layer for handling the text input, followed by a Long Short-Term Memory (LSTM) recurrent neural network layer.\n",
        "**Decoder** (for lack of a better name). Both the feature extractor and sequence processor output a fixed-length vector. These are merged together and processed by a Dense layer to make a final prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtmBOeZCwIOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = add([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summarize model\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VevwXDFwd42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define checkpoint callback\n",
        "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}